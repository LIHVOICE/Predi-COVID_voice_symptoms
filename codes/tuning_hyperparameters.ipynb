{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64941e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune(X_train, y_train, X_test, y_test, folder_out, n_jobs=1, random_state=1):\n",
    "    # X_train: array, training dataset which was used to select the features\n",
    "    # y_train: array, binary outcome with 0 and 1\n",
    "    # X_test: array, test dataset\n",
    "    # y_test: array, binary outcome with 0 and 1\n",
    "    # folder: string, folder to save the results\n",
    "    \n",
    "    # list of classifiers and hyperparameters\n",
    "    lst_clf = {}\n",
    "    lst_grid = {}\n",
    "\n",
    "    ##### Random forest #####\n",
    "    clf = RandomForestClassifier(n_jobs=n_jobs, random_state=random_state, class_weight='balanced')\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 100, stop = 400, num = 7)]\n",
    "    # The function to measure the quality of a split\n",
    "    criterion = ['gini', 'entropy']\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['sqrt','log2',0.2,0.4]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'clf__n_estimators': n_estimators,\n",
    "                   'clf__max_features': max_features,\n",
    "                   'clf__criterion': criterion,\n",
    "                   'clf__min_samples_leaf': min_samples_leaf}\n",
    "    lst_clf['rf'] = clf\n",
    "    lst_grid['rf'] = random_grid\n",
    "\n",
    "    ##### Support vector machine #####\n",
    "    clf = SVC(random_state=random_state, class_weight='balanced')\n",
    "    # Regularization parameter C\n",
    "    C = [100, 10, 1, 0.1, 0.001]\n",
    "    # kernel type to be used\n",
    "    kernel = ['linear','poly', 'rbf','sigmoid']\n",
    "    # Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n",
    "    gamma = ['scale','auto',0.001,0.1, 1, 10]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'clf__C': C,\n",
    "                   'clf__kernel': kernel,\n",
    "                   'clf__gamma': gamma}\n",
    "\n",
    "    lst_clf['svm'] = clf\n",
    "    lst_grid['svm'] = random_grid\n",
    "\n",
    "    ##### Bagging tree #####\n",
    "    clf = BaggingClassifier(random_state=random_state, n_jobs=n_jobs)\n",
    "\n",
    "    # The number of base estimators in the ensemble.\n",
    "    n_estimators = [10,100,500,1000]\n",
    "    # The number of features to draw from X to train each base estimator \n",
    "    max_features = [1,10]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'clf__n_estimators': n_estimators,\n",
    "                   'clf__max_features': max_features}\n",
    "\n",
    "    lst_clf['bagging_tree'] = clf\n",
    "    lst_grid['bagging_tree'] = random_grid\n",
    "    \n",
    "    ##### MLP #####\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    clf = MLPClassifier(random_state=random_state, max_iter=3000)\n",
    "\n",
    "    hidden_layer_sizes = [(10,),(20,)]\n",
    "    activation = ['tanh', 'relu']\n",
    "    solver = ['sgd', 'adam']\n",
    "    alpha = [0.0001, 0.001, 0.01]\n",
    "    learning_rate = ['constant','adaptive']\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {\n",
    "        'clf__hidden_layer_sizes': hidden_layer_sizes,\n",
    "        'clf__activation': activation,\n",
    "        'clf__solver': solver,\n",
    "        'clf__alpha': alpha,\n",
    "        'clf__learning_rate': learning_rate,\n",
    "    }\n",
    "\n",
    "    lst_clf['mlp'] = clf\n",
    "    lst_grid['mlp'] = random_grid\n",
    "    \n",
    "    ##### hyperparameter tuning on training dataset #####\n",
    "    for key, clf in lst_clf.items():\n",
    "        print(key)\n",
    "        pipeline = Pipeline([('scale', StandardScaler()),\n",
    "                                # (\"smote\", SMOTE(random_state=random_state, n_jobs=n_jobs)), \n",
    "                              (\"clf\", clf)])\n",
    "        # # pipeline.get_params().keys()\n",
    "\n",
    "        grid = lst_grid[key]\n",
    "        # df_best_params = pd.DataFrame()\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline, grid, scoring=metrics.make_scorer(metrics.matthews_corrcoef), n_iter=100,random_state=random_state, ###############\n",
    "            n_jobs=n_jobs, cv=kf,return_train_score=True\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "        best_score = search.best_score_\n",
    "        print(f\"Best Tuning MCC: {best_score}\") ###############\n",
    "\n",
    "        best_params = {\n",
    "            key: value for key, value in search.best_params_.items()\n",
    "        }\n",
    "        print(best_params)\n",
    "\n",
    "        best_estimator = search.best_estimator_\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        score_test = metrics.matthews_corrcoef(y_test, y_pred) ##################\n",
    "        print(f\"Best estimator MCC in test dataset: {score_test}\") ###################\n",
    "\n",
    "        df_ = pd.DataFrame.from_dict(best_params, orient='index').transpose()\n",
    "        df_['val_score'] = best_score\n",
    "        df_['test_score'] = score_test\n",
    "\n",
    "        path_out = os.path.join(folder_out, key+'.csv')\n",
    "        df_.to_csv(path_out, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
